{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as sd\n",
    "import sklearn.utils as su\n",
    "import sklearn.metrics as sm\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.preprocessing as sp\n",
    "import sklearn.pipeline as pl\n",
    "import sklearn.tree as st\n",
    "import sklearn.ensemble as se\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.svm as svm\n",
    "import sklearn.naive_bayes as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\n",
      " 1 Are you curious about tokenization?\n",
      " 2 Let's see how it works!\n",
      " 3 We need to analyze a couple of sentences with punctuations to see it in action.\n",
      "---------------\n",
      " 1 Are\n",
      " 2 you\n",
      " 3 curious\n",
      " 4 about\n",
      " 5 tokenization\n",
      " 6 ?\n",
      " 7 Let\n",
      " 8 's\n",
      " 9 see\n",
      "10 how\n",
      "11 it\n",
      "12 works\n",
      "13 !\n",
      "14 We\n",
      "15 need\n",
      "16 to\n",
      "17 analyze\n",
      "18 a\n",
      "19 couple\n",
      "20 of\n",
      "21 sentences\n",
      "22 with\n",
      "23 punctuations\n",
      "24 to\n",
      "25 see\n",
      "26 it\n",
      "27 in\n",
      "28 action\n",
      "29 .\n",
      "---------------\n",
      " 1 Are\n",
      " 2 you\n",
      " 3 curious\n",
      " 4 about\n",
      " 5 tokenization\n",
      " 6 ?\n",
      " 7 Let\n",
      " 8 '\n",
      " 9 s\n",
      "10 see\n",
      "11 how\n",
      "12 it\n",
      "13 works\n",
      "14 !\n",
      "15 We\n",
      "16 need\n",
      "17 to\n",
      "18 analyze\n",
      "19 a\n",
      "20 couple\n",
      "21 of\n",
      "22 sentences\n",
      "23 with\n",
      "24 punctuations\n",
      "25 to\n",
      "26 see\n",
      "27 it\n",
      "28 in\n",
      "29 action\n",
      "30 .\n"
     ]
    }
   ],
   "source": [
    "doc = \"Are you curious about tokenization? \" \\\n",
    "      \"Let's see how it works! \" \\\n",
    "      \"We need to analyze a couple of sentences \" \\\n",
    "      \"with punctuations to see it in action.\"\n",
    "print(doc)\n",
    "\n",
    "tokens = tk.sent_tokenize(doc)\n",
    "for i, token in enumerate(tokens):\n",
    "    print(\"%2d\" % (i + 1), token)\n",
    "print('-' * 15)\n",
    "\n",
    "tokens = tk.word_tokenize(doc)\n",
    "for i, token in enumerate(tokens):\n",
    "    print(\"%2d\" % (i + 1), token)\n",
    "print('-' * 15)\n",
    "\n",
    "tokenizer = tk.WordPunctTokenizer()\n",
    "tokens = tokenizer.tokenize(doc)\n",
    "for i, token in enumerate(tokens):\n",
    "    print(\"%2d\" % (i + 1), token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 1 0 0 0 1 0 1]\n",
      " [1 0 0 1 1 0 0 1 1 1 1 0]\n",
      " [0 1 1 1 0 1 1 0 1 1 0 1]]\n",
      "['bad', 'environment', 'good', 'hotel', 'in', 'is', 'of', 'smells', 'the', 'this', 'toilet', 'very']\n"
     ]
    }
   ],
   "source": [
    "doc = 'This hotel is very bad. The toilet in this hotel smells bad. The environment of this hotel is very good.'\n",
    "sentences = tk.sent_tokenize(doc)\n",
    "cv = ft.CountVectorizer()\n",
    "bow = cv.fit_transform(sentences).toarray()\n",
    "print(bow)\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.488 0.    0.    0.379 0.    0.488 0.    0.    0.    0.379 0.    0.488]\n",
      " [0.345 0.    0.    0.268 0.454 0.    0.    0.454 0.345 0.268 0.454 0.   ]\n",
      " [0.    0.429 0.429 0.253 0.    0.326 0.429 0.    0.326 0.253 0.    0.326]]\n"
     ]
    }
   ],
   "source": [
    "tt = ft.TfidfTransformer()\n",
    "tfidf = tt.fit_transform(bow).toarray()\n",
    "print(np.round(tfidf, 3))"
  